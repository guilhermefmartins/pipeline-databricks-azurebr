## âš™ï¸ OrquestraÃ§Ã£o de Pipelines com Databricks e Azure Data Factory (ImÃ³veis - Azure Data Lake)
Objetivo
Automatizar a execuÃ§Ã£o de notebooks no Azure Databricks utilizando o Azure Data Factory, com foco em pipelines de dados de imÃ³veis organizados nas camadas bronze e silver de um Data Lake na Azure, usando o formato de armazenamento Delta para garantir performance, escalabilidade e versionamento.

# ğŸ“Œ SumÃ¡rio
- [Contexto](#contexto)
- [Pipeline de Dados](#pipeline-de-dados)
- [Ferramentas Utilizadas](#ferramentas-utilizadas)
- [OrquestraÃ§Ã£o](#orquestraÃ§Ã£o)
- [Como Reproduzir](#como-reproduzir)
- [Resultados e ConclusÃµes](#resultados-e-conclusÃµes)

Contexto
O projeto teve como foco principal a construÃ§Ã£o de um pipeline de dados moderno e automatizado, com leitura, transformaÃ§Ã£o e persistÃªncia em Delta Lake, utilizando ferramentas de ponta do ecossistema Azure. Os dados simulam o mercado imobiliÃ¡rio e passam por diferentes camadas de tratamento com persistÃªncia eficiente.

# ğŸ— Pipeline de Dados 
Camadas estruturadas no Data Lake:

Bronze: IngestÃ£o bruta dos dados originais.

Silver: Dados tratados, limpos e prontos para anÃ¡lise e consumo por dashboards ou modelos de machine learning.

TransformaÃ§Ãµes aplicadas:

Leitura de arquivos originais em JSON

PadronizaÃ§Ã£o de nomes e tipos de colunas

Escrita dos dados tratados no formato Delta, garantindo versionamento, transaÃ§Ãµes ACID e performance superior para leitura e atualizaÃ§Ã£o

# ğŸ›  Ferramentas Utilizadas
Azure Databricks: Ambiente baseado em Apache Spark com notebooks em PySpark e Scala, usado para ingestÃ£o, tratamento e gravaÃ§Ã£o dos dados.

Delta Lake: Formato de armazenamento transacional utilizado para garantir consistÃªncia e eficiÃªncia nas camadas do Data Lake.

Azure Data Factory (ADF): Interface visual de orquestraÃ§Ã£o para pipelines, controle de execuÃ§Ã£o e integraÃ§Ã£o com serviÃ§os da Azure.

Azure Data Lake Storage Gen2: Armazenamento escalÃ¡vel e seguro para os dados, com suporte a hierarquia de diretÃ³rios e permissÃµes.

# ğŸ”„ OrquestraÃ§Ã£o
A orquestraÃ§Ã£o foi feita com o Azure Data Factory, utilizando pipelines visuais com agendamentos e monitoramento. As execuÃ§Ãµes foram programadas com triggers baseadas em horÃ¡rio e configuradas para rodar notebooks especÃ­ficos no Databricks. ADF gerenciou dependÃªncias, falhas e logs, garantindo rastreabilidade e controle no processo.

# ğŸ’» Como Reproduzir
Provisionar os serviÃ§os:

Azure Databricks

Azure Data Factory

Azure Data Lake Storage Gen2
<br/>
# Montar o Data Lake no Databricks:
```
val configs = Map(
  "fs.azure.account.auth.type" -> "OAuth",
  "fs.azure.account.oauth.provider.type" -> "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
  "fs.azure.account.oauth2.client.id" -> "<application-id>",
  "fs.azure.account.oauth2.client.secret" -> dbutils.secrets.get(scope="<scope-name>",key="<service-credential-key-name>"),
  "fs.azure.account.oauth2.client.endpoint" -> "https://login.microsoftonline.com/<directory-id>/oauth2/token")
// Optionally, you can add <directory-name> to the source URI of your mount point.
dbutils.fs.mount(
  source = "abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/",
  mountPoint = "/mnt/<mount-name>",
  extraConfigs = configs)
```
<br/>
# ğŸ“ˆ Resultados e ConclusÃµes
 <br/>
- âœ… Os dados foram processados com sucesso em um fluxo robusto e automatizado<br/>
- âœ… O uso do formato Delta permitiu maior controle, performance e confiabilidade nas camadas bronze e silver<br/>
- âœ… O Azure Data Factory ofereceu uma interface visual amigÃ¡vel para orquestraÃ§Ã£o, facilitando o agendamento e rastreamento das execuÃ§Ãµes<br/>
- âœ… O projeto demonstrou a integraÃ§Ã£o eficaz entre Databricks e os serviÃ§os do Azure, com organizaÃ§Ã£o modular e escalÃ¡vel<br/>
 <br/>

# ğŸ” Este projeto comprova minha capacidade de desenvolver pipelines completos e escalÃ¡veis em nuvem, combinando engenharia de dados moderna, automaÃ§Ã£o e boas prÃ¡ticas de orquestraÃ§Ã£o com ferramentas do ecossistema Azure.
